{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":18613,"sourceType":"datasetVersion","datasetId":5839},{"sourceId":14467223,"sourceType":"datasetVersion","datasetId":9240701}],"dockerImageVersionId":31236,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# ==================== CELL 1: Check Environment ====================\nimport os\nimport torch\nfrom pathlib import Path\n\n\nprint(\"=\"*70)\nprint(\"  Kaggle BLIP Training Environment Check\")\nprint(\"=\"*70)\n\n# Check GPU\nprint(f\"\\nPyTorch Version: {torch.__version__}\")\nprint(f\"CUDA Available: {torch.cuda.is_available()}\")\n\nif torch.cuda.is_available():\n    gpu_count = torch.cuda.device_count()\n    print(f\"Number of GPUs: {gpu_count}\")\n    for i in range(gpu_count):\n        print(f\"  GPU {i}: {torch.cuda.get_device_name(i)}\")\n        print(f\"  Memory: {torch.cuda.get_device_properties(i).total_memory / 1e9:.1f} GB\")\nelse:\n    print(\"‚ö†Ô∏è WARNING: No GPU detected!\")\n    print(\"Go to Settings ‚Üí Accelerator ‚Üí GPU T4 x2\")\n\n# Check Kaggle paths\nprint(f\"\\nKaggle Paths:\")\nprint(f\"  Working Dir: {os.getcwd()}\")\nprint(f\"  Input Dir: /kaggle/input/\")\nprint(f\"  Output Dir: /kaggle/working/\")\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-18T16:10:14.832064Z","iopub.execute_input":"2026-01-18T16:10:14.832334Z","iopub.status.idle":"2026-01-18T16:10:22.159865Z","shell.execute_reply.started":"2026-01-18T16:10:14.832309Z","shell.execute_reply":"2026-01-18T16:10:22.158969Z"}},"outputs":[{"name":"stdout","text":"======================================================================\n  Kaggle BLIP Training Environment Check\n======================================================================\n\nPyTorch Version: 2.8.0+cu126\nCUDA Available: True\nNumber of GPUs: 2\n  GPU 0: Tesla T4\n  Memory: 15.8 GB\n  GPU 1: Tesla T4\n  Memory: 15.8 GB\n\nKaggle Paths:\n  Working Dir: /kaggle/working\n  Input Dir: /kaggle/input/\n  Output Dir: /kaggle/working/\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# ==================== CELL 2: Install Dependencies ====================\nprint(\"\\nInstalling dependencies...\")\n\n# Kaggle has most packages, just need a few\n!pip install -q transformers==4.35.0 accelerate==0.24.0\n!pip install -q rouge-score nltk\n\nimport nltk\nnltk.download('punkt', quiet=True)\n\nprint(\"‚úÖ Dependencies installed!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-18T16:14:54.499848Z","iopub.execute_input":"2026-01-18T16:14:54.500907Z","iopub.status.idle":"2026-01-18T16:15:23.288919Z","shell.execute_reply.started":"2026-01-18T16:14:54.500856Z","shell.execute_reply":"2026-01-18T16:15:23.288171Z"}},"outputs":[{"name":"stdout","text":"\nInstalling dependencies...\n\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m123.1/123.1 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m7.9/7.9 MB\u001b[0m \u001b[31m78.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m261.0/261.0 kB\u001b[0m \u001b[31m14.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m96.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m295.0/295.0 kB\u001b[0m \u001b[31m14.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ndatasets 4.4.1 requires huggingface-hub<2.0,>=0.25.0, but you have huggingface-hub 0.17.3 which is incompatible.\nsentence-transformers 5.1.1 requires huggingface-hub>=0.20.0, but you have huggingface-hub 0.17.3 which is incompatible.\nsentence-transformers 5.1.1 requires transformers<5.0.0,>=4.41.0, but you have transformers 4.35.0 which is incompatible.\ngradio-client 1.13.3 requires huggingface-hub<2.0,>=0.19.3, but you have huggingface-hub 0.17.3 which is incompatible.\ngradio 5.49.1 requires huggingface-hub<2.0,>=0.33.5, but you have huggingface-hub 0.17.3 which is incompatible.\ngradio 5.49.1 requires pydantic<2.12,>=2.0, but you have pydantic 2.12.5 which is incompatible.\npeft 0.17.1 requires huggingface_hub>=0.25.0, but you have huggingface-hub 0.17.3 which is incompatible.\ndiffusers 0.35.2 requires huggingface-hub>=0.34.0, but you have huggingface-hub 0.17.3 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Building wheel for rouge-score (setup.py) ... \u001b[?25l\u001b[?25hdone\n‚úÖ Dependencies installed!\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"#==================== CELL 3: Checking Dataset=====================================\nfrom pathlib import Path\n\nprint(\"\\n\" + \"=\"*70)\nprint(\"  Locating NIH Dataset\")\nprint(\"=\"*70)\n\n# Common paths where NIH dataset might be\npossible_paths = [\n    '/kaggle/input/nih-chest-xrays/data/versions/3',\n    '/kaggle/input/data',\n    '/kaggle/input/chest-xray-dataset',\n    '/kaggle/input/nih-chest-xray-dataset',\n]\n\n# Search for Data_Entry_2017.csv\ndataset_path = None\nfor path in possible_paths:\n    if os.path.exists(path):\n        # Check if it has the required files\n        csv_file = None\n        for root, dirs, files in os.walk(path):\n            if 'Data_Entry_2017.csv' in files:\n                dataset_path = root\n                csv_file = os.path.join(root, 'Data_Entry_2017.csv')\n                break\n        if dataset_path:\n            break\n\n# If not found, search everywhere in /kaggle/input\nif not dataset_path:\n    print(\"Searching for dataset...\")\n    for root, dirs, files in os.walk('/kaggle/input'):\n        if 'Data_Entry_2017.csv' in files:\n            dataset_path = root\n            break\n\nif dataset_path:\n    print(f\"‚úÖ Dataset found at: {dataset_path}\")\n    \n    # List contents\n    print(f\"\\nDataset contents:\")\n    for item in os.listdir(dataset_path):\n        item_path = os.path.join(dataset_path, item)\n        if os.path.isdir(item_path):\n            count = len(list(Path(item_path).rglob('*.png'))) + len(list(Path(item_path).rglob('*.jpg')))\n            print(f\"  {item}/: {count} images\")\n        elif item.endswith('.csv') or item.endswith('.txt'):\n            print(f\"  {item}\")\nelse:\n    print(\"‚ùå Dataset not found!\")\n    print(\"\\nPlease add NIH Chest X-ray dataset:\")\n    print(\"  1. Click 'Add Data' (right panel)\")\n    print(\"  2. Search 'NIH Chest X-ray'\")\n    print(\"  3. Add to notebook\")\n    print(\"  4. Restart kernel\")\n    raise FileNotFoundError(\"NIH dataset not found\")\n\nDATASET_PATH = dataset_path","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-18T16:22:56.643274Z","iopub.execute_input":"2026-01-18T16:22:56.643916Z","iopub.status.idle":"2026-01-18T16:28:08.874413Z","shell.execute_reply.started":"2026-01-18T16:22:56.643880Z","shell.execute_reply":"2026-01-18T16:28:08.873514Z"}},"outputs":[{"name":"stdout","text":"\n======================================================================\n  Locating NIH Dataset\n======================================================================\n‚úÖ Dataset found at: /kaggle/input/data\n\nDataset contents:\n  images_003/: 10000 images\n  images_012/: 7121 images\n  BBox_List_2017.csv\n  images_009/: 10000 images\n  images_008/: 10000 images\n  images_007/: 10000 images\n  test_list.txt\n  images_010/: 10000 images\n  images_002/: 10000 images\n  images_011/: 10000 images\n  Data_Entry_2017.csv\n  images_001/: 4999 images\n  train_val_list.txt\n  images_005/: 10000 images\n  images_004/: 10000 images\n  images_006/: 10000 images\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"# ==================== CELL 4: Configuration ====================\nimport json\n\n# Training configuration optimized for Kaggle T4 x2\nconfig = {\n    # Dataset\n    'dataset_path': DATASET_PATH,\n    'output_dir': '/kaggle/working',\n    'sample_size': None,  # None = use all data, or set to number like 10000 for testing\n    \n    # Model\n    'model_name': 'Salesforce/blip-image-captioning-base',\n    'max_length': 256,\n    'image_size': 384,\n    \n    # Training\n    'batch_size': 12,  # Per GPU (total 48 with 2 GPUs)\n    'num_epochs': 3,  # Reduced for 9-hour limit of kaggle gpu usage limit\n    'learning_rate': 5e-5,\n    'weight_decay': 0.01,\n    'warmup_ratio': 0.1,\n    'gradient_accumulation_steps': 1,\n    'max_grad_norm': 1.0,\n    \n    # GPU settings\n    'use_multi_gpu': True,  # Use both GPUs if available\n    'mixed_precision': True,  # FP16 training\n    \n    # Checkpointing (important for 9-hour limit!)\n    'save_every_steps': 300,\n    'patience': 3,\n    'eval_every_epochs': 1,\n    \n    # Hardware\n    'num_workers': 4,\n    'pin_memory': True,\n}\n\n# Save config\nwith open('/kaggle/working/training_config.json', 'w') as f:\n    json.dump(config, f, indent=2)\n\nprint(\"‚úÖ Configuration created!\")\nprint(f\"\\nKey settings:\")\nprint(f\"  Dataset: {config['dataset_path']}\")\nprint(f\"  Batch size: {config['batch_size']} per GPU\")\nprint(f\"  Total batch: {config['batch_size'] * (2 if config['use_multi_gpu'] else 1)}\")\nprint(f\"  Epochs: {config['num_epochs']}\")\nprint(f\"  Sample size: {'ALL' if config['sample_size'] is None else config['sample_size']}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-18T16:28:59.717698Z","iopub.execute_input":"2026-01-18T16:28:59.718061Z","iopub.status.idle":"2026-01-18T16:28:59.726116Z","shell.execute_reply.started":"2026-01-18T16:28:59.718034Z","shell.execute_reply":"2026-01-18T16:28:59.725325Z"}},"outputs":[{"name":"stdout","text":"‚úÖ Configuration created!\n\nKey settings:\n  Dataset: /kaggle/input/data\n  Batch size: 12 per GPU\n  Total batch: 24\n  Epochs: 3\n  Sample size: ALL\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"# ==================== CELL 5: Data Preparation ====================\nprint(\"\\n\" + \"=\"*70)\nprint(\"  Preparing Dataset\")\nprint(\"=\"*70)\n\nimport pandas as pd\nimport shutil\nfrom tqdm.notebook import tqdm\nimport random\n\nclass NIHDatasetPreparator:\n    \"\"\"Prepare NIH dataset for BLIP training\"\"\"\n    \n    REPORT_TEMPLATES = {\n        'No Finding': [\n            \"Normal chest radiograph. No acute cardiopulmonary abnormality. The heart size is normal. The lungs are clear.\",\n            \"The heart size and mediastinal contours are normal. The lungs are clear. No pleural effusion or pneumothorax.\",\n        ],\n        'Atelectasis': [\n            \"Atelectasis present. Otherwise lungs are clear. No pleural effusion or pneumothorax.\",\n        ],\n        'Cardiomegaly': [\n            \"Cardiomegaly is present. The lungs are clear. No acute pulmonary abnormality.\",\n        ],\n        'Effusion': [\n            \"Pleural effusion noted. Otherwise clear lung fields. No pneumothorax.\",\n        ],\n        'Infiltration': [\n            \"Infiltrate present, possibly representing infection. Clinical correlation recommended.\",\n        ],\n        'Mass': [\n            \"Pulmonary mass identified. Recommend CT for further evaluation.\",\n        ],\n        'Nodule': [\n            \"Pulmonary nodule noted. Follow-up imaging recommended.\",\n        ],\n        'Pneumonia': [\n            \"Consolidation consistent with pneumonia. Clinical correlation recommended.\",\n        ],\n        'Pneumothorax': [\n            \"Pneumothorax present. Clinical correlation recommended.\",\n        ],\n        'Consolidation': [\n            \"Consolidation present. Clinical correlation recommended.\",\n        ],\n        'Edema': [\n            \"Pulmonary edema with prominent interstitial markings. Cardiomegaly present.\",\n        ],\n        'Emphysema': [\n            \"Emphysematous changes. Hyperinflation present. Heart size normal.\",\n        ],\n        'Fibrosis': [\n            \"Pulmonary fibrosis with reticular opacities. No acute process.\",\n        ],\n        'Pleural_Thickening': [\n            \"Pleural thickening. No acute abnormality. Lungs otherwise clear.\",\n        ],\n        'Hernia': [\n            \"Hiatal hernia present. Otherwise unremarkable chest radiograph.\",\n        ]\n    }\n    \n    def __init__(self, dataset_path, output_path):\n        self.dataset_path = Path(dataset_path)\n        self.output_path = Path(output_path)\n        self.output_path.mkdir(exist_ok=True)\n        \n    def generate_report(self, findings, view='PA'):\n        \"\"\"Generate report from findings\"\"\"\n        if pd.isna(findings) or findings == 'No Finding':\n            findings_list = ['No Finding']\n        else:\n            findings_list = findings.split('|')\n        \n        parts = [f\"{view} chest radiograph.\"]\n        \n        for finding in findings_list:\n            if finding in self.REPORT_TEMPLATES:\n                template = random.choice(self.REPORT_TEMPLATES[finding])\n                parts.append(template)\n        \n        report = ' '.join(parts)\n        impression = self._generate_impression(findings_list)\n        report += f\" Impression: {impression}\"\n        \n        return report\n    \n    def _generate_impression(self, findings):\n        if findings == ['No Finding']:\n            return \"No acute cardiopulmonary abnormality.\"\n        impressions = [f.replace('_', ' ') for f in findings if f in self.REPORT_TEMPLATES]\n        return ', '.join(impressions) + '.' if impressions else \"See findings above.\"\n    \n    def find_image(self, image_name):\n        \"\"\"Find image in dataset folders\"\"\"\n        for i in range(1, 13):\n            folder = self.dataset_path / f'images_{i:03d}' / 'images'\n            image_path = folder / image_name\n            if image_path.exists():\n                return image_path\n        return None\n    \n    def prepare(self, sample_size=None):\n        \"\"\"Prepare dataset\"\"\"\n        print(\"\\nLoading metadata...\")\n        df = pd.read_csv(self.dataset_path / 'Data_Entry_2017.csv')\n        \n        if sample_size:\n            df = df.sample(n=min(sample_size, len(df)), random_state=42)\n            print(f\"Using {len(df)} samples\")\n        else:\n            print(f\"Using all {len(df)} samples\")\n        \n        # Load splits\n        with open(self.dataset_path / 'train_val_list.txt', 'r') as f:\n            train_val_imgs = set(line.strip() for line in f)\n        with open(self.dataset_path / 'test_list.txt', 'r') as f:\n            test_imgs = set(line.strip() for line in f)\n        \n        # Split data\n        train_val_df = df[df['Image Index'].isin(train_val_imgs)]\n        test_df = df[df['Image Index'].isin(test_imgs)]\n        \n        from sklearn.model_selection import train_test_split\n        train_df, val_df = train_test_split(train_val_df, test_size=0.2, random_state=42)\n        \n        print(f\"\\nData split:\")\n        print(f\"  Train: {len(train_df)}\")\n        print(f\"  Val: {len(val_df)}\")\n        print(f\"  Test: {len(test_df)}\")\n        \n        # Create output directory\n        (self.output_path / 'images').mkdir(exist_ok=True)\n        \n        # Process each split\n        for split_name, split_df in [('train', train_df), ('val', val_df), ('test', test_df)]:\n            print(f\"\\nProcessing {split_name}...\")\n            \n            data = []\n            for _, row in tqdm(split_df.iterrows(), total=len(split_df)):\n                # Generate report\n                report = self.generate_report(row['Finding Labels'], row['View Position'])\n                \n                # Find image\n                img_name = row['Image Index']\n                src = self.find_image(img_name)\n                \n                if src is None:\n                    continue\n                \n                # Copy image (symlink to save space)\n                dst = self.output_path / 'images' / img_name\n                if not dst.exists():\n                    # Use symlink instead of copy to save space!\n                    try:\n                        os.symlink(src, dst)\n                    except:\n                        shutil.copy2(src, dst)\n                \n                data.append({'image_path': img_name, 'report': report})\n            \n            # Save CSV\n            pd.DataFrame(data).to_csv(self.output_path / f'{split_name}_data.csv', index=False)\n            print(f\"  ‚úì Saved {len(data)} samples\")\n        \n        print(f\"\\n‚úÖ Dataset prepared in {self.output_path}\")\n\n# Prepare dataset\npreparator = NIHDatasetPreparator(config['dataset_path'], config['output_dir'])\npreparator.prepare(sample_size=config['sample_size'])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-18T16:29:05.928428Z","iopub.execute_input":"2026-01-18T16:29:05.929148Z","iopub.status.idle":"2026-01-18T16:48:59.371020Z","shell.execute_reply.started":"2026-01-18T16:29:05.929120Z","shell.execute_reply":"2026-01-18T16:48:59.370296Z"}},"outputs":[{"name":"stdout","text":"\n======================================================================\n  Preparing Dataset\n======================================================================\n\nLoading metadata...\nUsing all 112120 samples\n\nData split:\n  Train: 69219\n  Val: 17305\n  Test: 25596\n\nProcessing train...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/69219 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9641bc605a7c4231b2927114cfc28246"}},"metadata":{}},{"name":"stdout","text":"  ‚úì Saved 69219 samples\n\nProcessing val...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/17305 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"20c15bf7dac44376a783c7870b57fe6d"}},"metadata":{}},{"name":"stdout","text":"  ‚úì Saved 17305 samples\n\nProcessing test...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/25596 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"81997ccdc08d4398bf9a445afde05bb9"}},"metadata":{}},{"name":"stdout","text":"  ‚úì Saved 25596 samples\n\n‚úÖ Dataset prepared in /kaggle/working\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"# ==================== CELL 6: Dataset and Model Classes ====================\nfrom torch.utils.data import Dataset, DataLoader\nfrom PIL import Image\nfrom transformers import BlipProcessor, BlipForConditionalGeneration\n\nclass XrayDataset(Dataset):\n    def __init__(self, csv_file, image_dir, processor, max_length=256):\n        self.data = pd.read_csv(csv_file)\n        self.image_dir = image_dir\n        self.processor = processor\n        self.max_length = max_length\n        print(f\"Loaded {len(self.data)} samples\")\n    \n    def __len__(self):\n        return len(self.data)\n    \n    def __getitem__(self, idx):\n        row = self.data.iloc[idx]\n        image_path = os.path.join(self.image_dir, row['image_path'])\n        \n        try:\n            image = Image.open(image_path).convert('RGB')\n        except:\n            image = Image.new('RGB', (384, 384), color='gray')\n        \n        report = str(row['report'])\n        \n        encoding = self.processor(\n            images=image,\n            text=report,\n            return_tensors=\"pt\",\n            padding=\"max_length\",\n            max_length=self.max_length,\n            truncation=True\n        )\n        return {k: v.squeeze(0) for k, v in encoding.items()}\n\nprint(\"‚úÖ Dataset class defined\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-18T16:49:06.331325Z","iopub.execute_input":"2026-01-18T16:49:06.332188Z","iopub.status.idle":"2026-01-18T16:49:18.619240Z","shell.execute_reply.started":"2026-01-18T16:49:06.332149Z","shell.execute_reply":"2026-01-18T16:49:18.618329Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.12/dist-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n  _torch_pytree._register_pytree_node(\n/usr/local/lib/python3.12/dist-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n  _torch_pytree._register_pytree_node(\n","output_type":"stream"},{"name":"stdout","text":"‚úÖ Dataset class defined\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.12/dist-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n  _torch_pytree._register_pytree_node(\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"# ==================== CELL 7: Training Setup ====================\nprint(\"\\n\" + \"=\"*70)\nprint(\"  Setting Up Training\")\nprint(\"=\"*70)\n\n# Load model\nLOCAL_MODEL_PATH = \"/kaggle/input/blip-model/blip-base\"\n\n# In training setup\nprocessor = BlipProcessor.from_pretrained(LOCAL_MODEL_PATH)\nmodel = BlipForConditionalGeneration.from_pretrained(\n    LOCAL_MODEL_PATH,\n    local_files_only=True\n)\n# Multi-GPU setup\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nif config['use_multi_gpu'] and torch.cuda.device_count() > 1:\n    print(f\"Using {torch.cuda.device_count()} GPUs with DataParallel\")\n    model = torch.nn.DataParallel(model)\n    \nmodel = model.to(device)\n\ntotal_params = sum(p.numel() for p in model.parameters()) / 1e6\nprint(f\"‚úÖ Model loaded: {total_params:.1f}M parameters\")\n\n# Create datasets\nprint(\"\\nCreating dataloaders...\")\ntrain_dataset = XrayDataset(\n    f\"{config['output_dir']}/train_data.csv\",\n    f\"{config['output_dir']}/images\",\n    processor,\n    config['max_length']\n)\nval_dataset = XrayDataset(\n    f\"{config['output_dir']}/val_data.csv\",\n    f\"{config['output_dir']}/images\",\n    processor,\n    config['max_length']\n)\n\ntrain_loader = DataLoader(\n    train_dataset,\n    batch_size=config['batch_size'],\n    shuffle=True,\n    num_workers=config['num_workers'],\n    pin_memory=config['pin_memory']\n)\nval_loader = DataLoader(\n    val_dataset,\n    batch_size=config['batch_size'],\n    shuffle=False,\n    num_workers=config['num_workers'],\n    pin_memory=config['pin_memory']\n)\n\nprint(f\"‚úì Train: {len(train_dataset)} samples ({len(train_loader)} batches)\")\nprint(f\"‚úì Val: {len(val_dataset)} samples ({len(val_loader)} batches)\")\n\n# Optimizer\nfrom torch.optim import AdamW\nfrom transformers import get_cosine_schedule_with_warmup  \n\noptimizer = AdamW(\n    model.parameters(),\n    lr=config['learning_rate'],\n    weight_decay=config['weight_decay']\n)\n\nnum_training_steps = len(train_loader) * config['num_epochs']\nnum_warmup_steps = int(num_training_steps * config['warmup_ratio'])\n\nscheduler = get_cosine_schedule_with_warmup(\n    optimizer,\n    num_warmup_steps=num_warmup_steps,\n    num_training_steps=num_training_steps\n)\n\nprint(f\"\\n‚úÖ Training setup complete!\")\nprint(f\"  Total steps: {num_training_steps}\")\nprint(f\"  Warmup steps: {num_warmup_steps}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-16T08:11:14.811077Z","iopub.execute_input":"2026-01-16T08:11:14.811506Z","iopub.status.idle":"2026-01-16T08:11:45.432976Z","shell.execute_reply.started":"2026-01-16T08:11:14.811479Z","shell.execute_reply":"2026-01-16T08:11:45.432156Z"}},"outputs":[{"name":"stderr","text":"2026-01-16 08:11:18.833310: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1768551079.350073      55 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1768551079.495058      55 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nW0000 00:00:1768551080.724784      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1768551080.724829      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1768551080.724832      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1768551080.724834      ","output_type":"stream"},{"name":"stdout","text":"\n======================================================================\n  Setting Up Training\n======================================================================\n","output_type":"stream"},{"name":"stderr","text":"55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n","output_type":"stream"},{"name":"stdout","text":"Using 2 GPUs with DataParallel\n‚úÖ Model loaded: 247.4M parameters\n\nCreating dataloaders...\nLoaded 69219 samples\nLoaded 17305 samples\n‚úì Train: 69219 samples (5769 batches)\n‚úì Val: 17305 samples (1443 batches)\n\n‚úÖ Training setup complete!\n  Total steps: 17307\n  Warmup steps: 1730\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"#==================Cell 8: Always ON Display ==================================\nfrom IPython.display import Javascript, display\n\n# Keep session alive by simulating activity\ndef keep_alive():\n    display(Javascript('''\n        function KeepClicking(){\n            console.log(\"Keeping session alive...\");\n            document.querySelector('body').click();\n        }\n        setInterval(KeepClicking, 60000); // Click every 60 seconds\n    '''))\n\nkeep_alive()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-18T16:49:29.813258Z","iopub.execute_input":"2026-01-18T16:49:29.813782Z","iopub.status.idle":"2026-01-18T16:49:29.820485Z","shell.execute_reply.started":"2026-01-18T16:49:29.813736Z","shell.execute_reply":"2026-01-18T16:49:29.819729Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.Javascript object>","application/javascript":"\n        function KeepClicking(){\n            console.log(\"Keeping session alive...\");\n            document.querySelector('body').click();\n        }\n        setInterval(KeepClicking, 60000); // Click every 60 seconds\n    "},"metadata":{}}],"execution_count":7},{"cell_type":"code","source":"# ==================== CELL 9: Training Loop ====================\nimport time\n\nprint(\"\\n\" + \"=\"*70)\nprint(\"  TRAINING STARTED\")\nprint(\"=\"*70)\n\nbest_val_loss = float('inf')\npatience_counter = 0\nglobal_step = 0\n\n# Create checkpoint directory\ncheckpoint_dir = Path('/kaggle/working/checkpoints')\ncheckpoint_dir.mkdir(exist_ok=True)\n\nfor epoch in range(1, config['num_epochs'] + 1):\n    print(f\"\\nEpoch {epoch}/{config['num_epochs']}\")\n    print(\"-\"*70)\n    \n    # Training\n    model.train()\n    train_loss = 0\n    start_time = time.time()\n    \n    progress = tqdm(train_loader, desc=\"Training\")\n    for batch_idx, batch in enumerate(progress):\n        pixel_values = batch['pixel_values'].to(device)\n        input_ids = batch['input_ids'].to(device)\n        \n        outputs = model(pixel_values=pixel_values, input_ids=input_ids, labels=input_ids)\n        \n        # Handle DataParallel output\n        loss = outputs.loss.mean() if hasattr(outputs.loss, 'mean') else outputs.loss\n        \n        optimizer.zero_grad()\n        loss.backward()\n        torch.nn.utils.clip_grad_norm_(model.parameters(), config['max_grad_norm'])\n        optimizer.step()\n        scheduler.step()\n        \n        train_loss += loss.item()\n        global_step += 1\n        \n        progress.set_postfix({'loss': f'{loss.item():.4f}', 'lr': f'{scheduler.get_last_lr()[0]:.2e}'})\n        \n        # Save checkpoint periodically\n        if global_step % config['save_every_steps'] == 0:\n            checkpoint_path = checkpoint_dir / f'checkpoint_step_{global_step}.pt'\n           # Save only essential state (smaller file)\n            torch.save({\n                'epoch': epoch,\n                'global_step': global_step,\n                'model_state_dict': model.module.state_dict() if hasattr(model, 'module') else model.state_dict(),\n                'best_val_loss': best_val_loss,\n            }, checkpoint_path)\n            print(f\"\\n  üíæ Checkpoint saved: step {global_step}\")\n            # Keep only last 2 checkpoints to save space\n            checkpoints = sorted(checkpoint_dir.glob('checkpoint_step_*.pt'))\n            if len(checkpoints) > 2:\n                for old_ckpt in checkpoints[:-2]:\n                    old_ckpt.unlink()\n\n    \n    \n    avg_train_loss = train_loss / len(train_loader)\n    epoch_time = time.time() - start_time\n    \n    print(f\"  Train Loss: {avg_train_loss:.4f}\")\n    print(f\"  Time: {epoch_time/60:.1f} min\")\n    \n    # Validation\n    if epoch % config['eval_every_epochs'] == 0:\n        model.eval()\n        val_loss = 0\n        \n        with torch.no_grad():\n            for batch in tqdm(val_loader, desc=\"Validating\"):\n                pixel_values = batch['pixel_values'].to(device)\n                input_ids = batch['input_ids'].to(device)\n                \n                outputs = model(pixel_values=pixel_values, input_ids=input_ids, labels=input_ids)\n                loss = outputs.loss.mean() if hasattr(outputs.loss, 'mean') else outputs.loss\n                val_loss += loss.item()\n        \n        avg_val_loss = val_loss / len(val_loader)\n        print(f\"  Val Loss: {avg_val_loss:.4f}\")\n        \n        # Save best model\n        if avg_val_loss < best_val_loss:\n            best_val_loss = avg_val_loss\n            patience_counter = 0\n            \n            best_model_dir = checkpoint_dir / 'best_model'\n            best_model_dir.mkdir(exist_ok=True)\n            \n            # Save model (handle DataParallel)\n            model_to_save = model.module if hasattr(model, 'module') else model\n            model_to_save.save_pretrained(best_model_dir)\n            processor.save_pretrained(best_model_dir)\n            \n            print(f\"  üèÜ Best model saved! Val Loss: {best_val_loss:.4f}\")\n        else:\n            patience_counter += 1\n            print(f\"  No improvement. Patience: {patience_counter}/{config['patience']}\")\n        \n        # Early stopping\n        if patience_counter >= config['patience']:\n            print(\"\\n‚ö†Ô∏è Early stopping!\")\n            break\n\nprint(\"\\n\" + \"=\"*70)\nprint(\"  TRAINING COMPLETE!\")\nprint(\"=\"*70)\nprint(f\"Best Val Loss: {best_val_loss:.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-16T08:11:45.441370Z","iopub.execute_input":"2026-01-16T08:11:45.441597Z","iopub.status.idle":"2026-01-16T16:31:59.971434Z","shell.execute_reply.started":"2026-01-16T08:11:45.441574Z","shell.execute_reply":"2026-01-16T16:31:59.966293Z"}},"outputs":[{"name":"stdout","text":"\n======================================================================\n  TRAINING STARTED\n======================================================================\n\nEpoch 1/3\n----------------------------------------------------------------------\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/5769 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"51cc7824f9a54c9983bdb9c8d0763a34"}},"metadata":{}},{"name":"stderr","text":"We strongly recommend passing in an `attention_mask` since your input_ids may be padded. See https://huggingface.co/docs/transformers/troubleshooting#incorrect-output-when-padding-tokens-arent-masked.\n/usr/local/lib/python3.12/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"\n  üíæ Checkpoint saved: step 300\n\n  üíæ Checkpoint saved: step 600\n\n  üíæ Checkpoint saved: step 900\n\n  üíæ Checkpoint saved: step 1200\n\n  üíæ Checkpoint saved: step 1500\n\n  üíæ Checkpoint saved: step 1800\n\n  üíæ Checkpoint saved: step 2100\n\n  üíæ Checkpoint saved: step 2400\n\n  üíæ Checkpoint saved: step 2700\n\n  üíæ Checkpoint saved: step 3000\n\n  üíæ Checkpoint saved: step 3300\n\n  üíæ Checkpoint saved: step 3600\n\n  üíæ Checkpoint saved: step 3900\n\n  üíæ Checkpoint saved: step 4200\n\n  üíæ Checkpoint saved: step 4500\n\n  üíæ Checkpoint saved: step 4800\n\n  üíæ Checkpoint saved: step 5100\n\n  üíæ Checkpoint saved: step 5400\n\n  üíæ Checkpoint saved: step 5700\n  Train Loss: 1.7498\n  Time: 152.5 min\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Validating:   0%|          | 0/1443 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9deca7e83f1747ceb0d24aef07e63233"}},"metadata":{}},{"name":"stdout","text":"  Val Loss: 1.3668\n  üèÜ Best model saved! Val Loss: 1.3668\n\nEpoch 2/3\n----------------------------------------------------------------------\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/5769 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6f034de09ebd464c8a4a8b521e185184"}},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.12/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"\n  üíæ Checkpoint saved: step 6000\n\n  üíæ Checkpoint saved: step 6300\n\n  üíæ Checkpoint saved: step 6600\n\n  üíæ Checkpoint saved: step 6900\n\n  üíæ Checkpoint saved: step 7200\n\n  üíæ Checkpoint saved: step 7500\n\n  üíæ Checkpoint saved: step 7800\n\n  üíæ Checkpoint saved: step 8100\n\n  üíæ Checkpoint saved: step 8400\n\n  üíæ Checkpoint saved: step 8700\n\n  üíæ Checkpoint saved: step 9000\n\n  üíæ Checkpoint saved: step 9300\n\n  üíæ Checkpoint saved: step 9600\n\n  üíæ Checkpoint saved: step 9900\n\n  üíæ Checkpoint saved: step 10200\n\n  üíæ Checkpoint saved: step 10500\n\n  üíæ Checkpoint saved: step 10800\n\n  üíæ Checkpoint saved: step 11100\n\n  üíæ Checkpoint saved: step 11400\n  Train Loss: 1.3663\n  Time: 152.9 min\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Validating:   0%|          | 0/1443 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1d00a35296eb4d6fa9581c2208cf1d5d"}},"metadata":{}},{"name":"stdout","text":"  Val Loss: 1.3660\n  üèÜ Best model saved! Val Loss: 1.3660\n\nEpoch 3/3\n----------------------------------------------------------------------\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/5769 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c461bcdedd964228af5f2cf386439b63"}},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.12/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"\n  üíæ Checkpoint saved: step 11700\n\n  üíæ Checkpoint saved: step 12000\n\n  üíæ Checkpoint saved: step 12300\n\n  üíæ Checkpoint saved: step 12600\n\n  üíæ Checkpoint saved: step 12900\n\n  üíæ Checkpoint saved: step 13200\n\n  üíæ Checkpoint saved: step 13500\n\n  üíæ Checkpoint saved: step 13800\n\n  üíæ Checkpoint saved: step 14100\n\n  üíæ Checkpoint saved: step 14400\n\n  üíæ Checkpoint saved: step 14700\n\n  üíæ Checkpoint saved: step 15000\n\n  üíæ Checkpoint saved: step 15300\n\n  üíæ Checkpoint saved: step 15600\n\n  üíæ Checkpoint saved: step 15900\n\n  üíæ Checkpoint saved: step 16200\n\n  üíæ Checkpoint saved: step 16500\n\n  üíæ Checkpoint saved: step 16800\n\n  üíæ Checkpoint saved: step 17100\n  Train Loss: 1.3654\n  Time: 152.8 min\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Validating:   0%|          | 0/1443 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0c0ad229e49c4c8d89bb90402fe19331"}},"metadata":{}},{"name":"stdout","text":"  Val Loss: 1.3657\n  üèÜ Best model saved! Val Loss: 1.3657\n\n======================================================================\n  TRAINING COMPLETE!\n======================================================================\nBest Val Loss: 1.3657\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"# Check the best_model directory\nbest_model_dir = Path('/kaggle/working/checkpoints')\nprint(\"Contents of best_model/:\")\nfor item in best_model_dir.iterdir():\n    if item.is_file():\n        size_mb = item.stat().st_size / (1024**2)\n        print(f\"  üìÑ {item.name} ({size_mb:.2f} MB)\")\n    else:\n        print(f\"  üìÅ {item.name}/\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-16T17:01:58.347382Z","iopub.execute_input":"2026-01-16T17:01:58.348014Z","iopub.status.idle":"2026-01-16T17:01:58.353699Z","shell.execute_reply.started":"2026-01-16T17:01:58.347985Z","shell.execute_reply":"2026-01-16T17:01:58.352905Z"}},"outputs":[{"name":"stdout","text":"Contents of best_model/:\n  üìÅ best_model/\n  üìÑ checkpoint_step_9900.pt (944.01 MB)\n  üìÑ checkpoint_step_9600.pt (944.01 MB)\n","output_type":"stream"}],"execution_count":37},{"cell_type":"code","source":"!zip -r /kaggle/working/best_model.zip /kaggle/working/checkpoints\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-16T16:37:03.007712Z","iopub.execute_input":"2026-01-16T16:37:03.008027Z","iopub.status.idle":"2026-01-16T16:39:37.735429Z","shell.execute_reply.started":"2026-01-16T16:37:03.007999Z","shell.execute_reply":"2026-01-16T16:39:37.734701Z"}},"outputs":[{"name":"stdout","text":"  adding: kaggle/working/checkpoints/ (stored 0%)\n  adding: kaggle/working/checkpoints/best_model/ (stored 0%)\n  adding: kaggle/working/checkpoints/best_model/tokenizer_config.json (deflated 74%)\n  adding: kaggle/working/checkpoints/best_model/tokenizer.json (deflated 71%)\n  adding: kaggle/working/checkpoints/best_model/preprocessor_config.json (deflated 48%)\n  adding: kaggle/working/checkpoints/best_model/generation_config.json (deflated 28%)\n  adding: kaggle/working/checkpoints/best_model/config.json (deflated 52%)\n  adding: kaggle/working/checkpoints/best_model/model.safetensors (deflated 7%)\n  adding: kaggle/working/checkpoints/best_model/special_tokens_map.json (deflated 42%)\n  adding: kaggle/working/checkpoints/best_model/vocab.txt (deflated 53%)\n  adding: kaggle/working/checkpoints/checkpoint_step_9900.pt (deflated 7%)\n  adding: kaggle/working/checkpoints/checkpoint_step_9600.pt (deflated 7%)\n","output_type":"stream"}],"execution_count":19},{"cell_type":"code","source":"!pip install -q huggingface_hub transformers safetensors\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-16T16:59:55.431551Z","iopub.execute_input":"2026-01-16T16:59:55.432326Z","iopub.status.idle":"2026-01-16T17:00:04.433062Z","shell.execute_reply.started":"2026-01-16T16:59:55.432267Z","shell.execute_reply":"2026-01-16T17:00:04.432238Z"}},"outputs":[],"execution_count":33},{"cell_type":"code","source":"# Login to HuggingFace\nfrom huggingface_hub import login\nlogin()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-16T17:00:40.254623Z","iopub.execute_input":"2026-01-16T17:00:40.255344Z","iopub.status.idle":"2026-01-16T17:00:40.269679Z","shell.execute_reply.started":"2026-01-16T17:00:40.255311Z","shell.execute_reply":"2026-01-16T17:00:40.269085Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv‚Ä¶","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4eef7d7a7fb34515bf5b5bdfc7241295"}},"metadata":{}}],"execution_count":36},{"cell_type":"code","source":"from huggingface_hub import HfApi\nfrom pathlib import Path\n\nrepo_id = \"anassaifi8912/chestxray-blip-report-generator\"\nmodel_dir = Path(\"/kaggle/working/checkpoints/\")\n\napi = HfApi()\n\napi.create_repo(\n    repo_id=repo_id,\n    repo_type=\"model\",\n    exist_ok=True\n)\n\napi.upload_folder(\n    folder_path=model_dir,\n    repo_id=repo_id,\n    repo_type=\"model\"\n)\n\nprint(\"‚úÖ Model successfully uploaded to Hugging Face!\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-16T17:03:21.901904Z","iopub.execute_input":"2026-01-16T17:03:21.902672Z","iopub.status.idle":"2026-01-16T17:03:59.457217Z","shell.execute_reply.started":"2026-01-16T17:03:21.902639Z","shell.execute_reply":"2026-01-16T17:03:59.456474Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"checkpoint_step_9900.pt:   0%|          | 0.00/990M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"30efce29cffc4c50bdd729e77fff7faf"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/990M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b7521b9b5ad241c28989c7a50b3985a1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"checkpoint_step_9600.pt:   0%|          | 0.00/990M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"86b259d6413d402c9ffb44d47e119cbf"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Upload 3 LFS files:   0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a16eae225d1c47fa936764876ef619f5"}},"metadata":{}},{"name":"stdout","text":"‚úÖ Model successfully uploaded to Hugging Face!\n","output_type":"stream"}],"execution_count":39},{"cell_type":"code","source":"!pip install kaggle","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-19T09:25:54.716090Z","iopub.execute_input":"2026-01-19T09:25:54.716338Z","iopub.status.idle":"2026-01-19T09:25:59.282586Z","shell.execute_reply.started":"2026-01-19T09:25:54.716316Z","shell.execute_reply":"2026-01-19T09:25:59.281741Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: kaggle in /usr/local/lib/python3.12/dist-packages (1.7.4.5)\nRequirement already satisfied: bleach in /usr/local/lib/python3.12/dist-packages (from kaggle) (6.2.0)\nRequirement already satisfied: certifi>=14.05.14 in /usr/local/lib/python3.12/dist-packages (from kaggle) (2025.11.12)\nRequirement already satisfied: charset-normalizer in /usr/local/lib/python3.12/dist-packages (from kaggle) (3.4.4)\nRequirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from kaggle) (3.11)\nRequirement already satisfied: protobuf in /usr/local/lib/python3.12/dist-packages (from kaggle) (5.29.5)\nRequirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.12/dist-packages (from kaggle) (2.9.0.post0)\nRequirement already satisfied: python-slugify in /usr/local/lib/python3.12/dist-packages (from kaggle) (8.0.4)\nRequirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from kaggle) (2.32.5)\nRequirement already satisfied: setuptools>=21.0.0 in /usr/local/lib/python3.12/dist-packages (from kaggle) (75.2.0)\nRequirement already satisfied: six>=1.10 in /usr/local/lib/python3.12/dist-packages (from kaggle) (1.17.0)\nRequirement already satisfied: text-unidecode in /usr/local/lib/python3.12/dist-packages (from kaggle) (1.3)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from kaggle) (4.67.1)\nRequirement already satisfied: urllib3>=1.15.1 in /usr/local/lib/python3.12/dist-packages (from kaggle) (2.6.2)\nRequirement already satisfied: webencodings in /usr/local/lib/python3.12/dist-packages (from kaggle) (0.5.1)\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}