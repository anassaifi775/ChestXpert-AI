{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":18613,"sourceType":"datasetVersion","datasetId":5839}],"dockerImageVersionId":31260,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# CELL 1: Setup & Installation\n# ================================================================\n\nprint(\"=\"*70)\nprint(\"  SETTING UP ENVIRONMENT\")\nprint(\"=\"*70)\n\n!pip install -q transformers torch torchvision pillow nltk rouge-score huggingface-hub\n\nimport torch\nprint(f\"\\n‚úÖ PyTorch version: {torch.__version__}\")\nprint(f\"‚úÖ CUDA available: {torch.cuda.is_available()}\")\nprint(f\"‚úÖ GPU count: {torch.cuda.device_count()}\")\n\nif torch.cuda.is_available():\n    for i in range(torch.cuda.device_count()):\n        print(f\"‚úÖ GPU {i}: {torch.cuda.get_device_name(i)}\")\n        print(f\"   Memory: {torch.cuda.get_device_properties(i).total_memory / 1024**3:.1f} GB\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2026-01-19T05:24:21.191070Z","iopub.execute_input":"2026-01-19T05:24:21.191794Z","iopub.status.idle":"2026-01-19T05:24:29.138775Z","shell.execute_reply.started":"2026-01-19T05:24:21.191762Z","shell.execute_reply":"2026-01-19T05:24:29.138081Z"}},"outputs":[{"name":"stdout","text":"======================================================================\n  SETTING UP ENVIRONMENT\n======================================================================\n\n‚úÖ PyTorch version: 2.8.0+cu126\n‚úÖ CUDA available: True\n‚úÖ GPU count: 2\n‚úÖ GPU 0: Tesla T4\n   Memory: 14.7 GB\n‚úÖ GPU 1: Tesla T4\n   Memory: 14.7 GB\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"#==================== CELL 3: Checking Dataset=====================================\nfrom pathlib import Path\nimport os\n\nprint(\"\\n\" + \"=\"*70)\nprint(\"  Locating NIH Dataset\")\nprint(\"=\"*70)\n\n# Common paths where NIH dataset might be\npossible_paths = [\n    '/kaggle/input/nih-chest-xrays/data/versions/3',\n    '/kaggle/input/data',\n    '/kaggle/input/chest-xray-dataset',\n    '/kaggle/input/nih-chest-xray-dataset',\n]\n\n# Search for Data_Entry_2017.csv\ndataset_path = None\nfor path in possible_paths:\n    if os.path.exists(path):\n        # Check if it has the required files\n        csv_file = None\n        for root, dirs, files in os.walk(path):\n            if 'Data_Entry_2017.csv' in files:\n                dataset_path = root\n                csv_file = os.path.join(root, 'Data_Entry_2017.csv')\n                break\n        if dataset_path:\n            break\n\n# If not found, search everywhere in /kaggle/input\nif not dataset_path:\n    print(\"Searching for dataset...\")\n    for root, dirs, files in os.walk('/kaggle/input'):\n        if 'Data_Entry_2017.csv' in files:\n            dataset_path = root\n            break\n\nif dataset_path:\n    print(f\"‚úÖ Dataset found at: {dataset_path}\")\n    \n    # List contents\n    print(f\"\\nDataset contents:\")\n    for item in os.listdir(dataset_path):\n        item_path = os.path.join(dataset_path, item)\n        if os.path.isdir(item_path):\n            count = len(list(Path(item_path).rglob('*.png'))) + len(list(Path(item_path).rglob('*.jpg')))\n            print(f\"  {item}/: {count} images\")\n        elif item.endswith('.csv') or item.endswith('.txt'):\n            print(f\"  {item}\")\nelse:\n    print(\"‚ùå Dataset not found!\")\n    print(\"\\nPlease add NIH Chest X-ray dataset:\")\n    print(\"  1. Click 'Add Data' (right panel)\")\n    print(\"  2. Search 'NIH Chest X-ray'\")\n    print(\"  3. Add to notebook\")\n    print(\"  4. Restart kernel\")\n    raise FileNotFoundError(\"NIH dataset not found\")\n\nDATASET_PATH = dataset_path","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-19T05:24:34.461204Z","iopub.execute_input":"2026-01-19T05:24:34.461950Z","iopub.status.idle":"2026-01-19T05:27:10.703458Z","shell.execute_reply.started":"2026-01-19T05:24:34.461919Z","shell.execute_reply":"2026-01-19T05:27:10.702752Z"}},"outputs":[{"name":"stdout","text":"\n======================================================================\n  Locating NIH Dataset\n======================================================================\n‚úÖ Dataset found at: /kaggle/input/data\n\nDataset contents:\n  images_003/: 10000 images\n  images_012/: 7121 images\n  BBox_List_2017.csv\n  images_009/: 10000 images\n  images_008/: 10000 images\n  images_007/: 10000 images\n  test_list.txt\n  images_010/: 10000 images\n  images_002/: 10000 images\n  images_011/: 10000 images\n  Data_Entry_2017.csv\n  images_001/: 4999 images\n  train_val_list.txt\n  images_005/: 10000 images\n  images_004/: 10000 images\n  images_006/: 10000 images\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"# CELL 2: Configuration\n# ================================================================\n\nprint(\"\\n\" + \"=\"*70)\nprint(\"  CONFIGURATION\")\nprint(\"=\"*70)\n\nconfig = {\n    # Dataset\n    'dataset_path': DATASET_PATH,\n    'output_dir': '/kaggle/working',\n    'sample_size': None,  # None = use all data, or set to number like 10000 for testing\n}\n# Settings\nBATCH_SIZE = 12     # Large batch for T4 X2\nNUM_WORKERS = 4     # Parallel loading\nUSE_FP16 = True     # Faster inference\n\n# ‚ö†Ô∏è UPDATE THESE!\nHUGGINGFACE_MODEL_ID = \"anassaifi8912/chestxray-blip-report-generator\"  # Your HF model\nHF_TOKEN = None  # Set to \"hf_xxxxx\" if model is private, or None if public\n\n# Test data path\nTEST_DATASET_PATH = \"/kaggle/input/nih-chest-xrays\"  # NIH dataset\nTEST_JSON = \"/kaggle/working/test_data.json\"  # If you already have test JSON\n\n# Output directory\nOUTPUT_DIR = \"/kaggle/working/test_results\"\n\n\nprint(f\"\\nüìã Settings:\")\nprint(f\"  Model: {HUGGINGFACE_MODEL_ID}\")\nprint(f\"  Test data: {TEST_DATASET_PATH}\")\nprint(f\"  Batch size: {BATCH_SIZE}\")\nprint(f\"  FP16: {USE_FP16}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-19T05:27:10.745507Z","iopub.execute_input":"2026-01-19T05:27:10.745693Z","iopub.status.idle":"2026-01-19T05:27:10.760120Z","shell.execute_reply.started":"2026-01-19T05:27:10.745675Z","shell.execute_reply":"2026-01-19T05:27:10.759508Z"}},"outputs":[{"name":"stdout","text":"\n======================================================================\n  CONFIGURATION\n======================================================================\n\nüìã Settings:\n  Model: anassaifi8912/chestxray-blip-report-generator\n  Test data: /kaggle/input/nih-chest-xrays\n  Batch size: 12\n  FP16: True\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"# ==================== CELL 5: Test Data Preparation ONLY ====================\nprint(\"\\n\" + \"=\"*70)\nprint(\"  Preparing TEST Dataset Only\")\nprint(\"=\"*70)\n\nimport pandas as pd\nimport shutil\nfrom tqdm.notebook import tqdm\nimport random\nfrom pathlib import Path\nimport os\n\nclass NIHTestDataPreparator:\n    \"\"\"Prepare ONLY test data from NIH dataset for final evaluation\"\"\"\n    \n    # Same templates as your training data - no extra content added\n    REPORT_TEMPLATES = {\n        'No Finding': [\n            \"Normal chest radiograph. No acute cardiopulmonary abnormality. The heart size is normal. The lungs are clear.\",\n            \"The heart size and mediastinal contours are normal. The lungs are clear. No pleural effusion or pneumothorax.\",\n        ],\n        'Atelectasis': [\n            \"Atelectasis present. Otherwise lungs are clear. No pleural effusion or pneumothorax.\",\n        ],\n        'Cardiomegaly': [\n            \"Cardiomegaly is present. The lungs are clear. No acute pulmonary abnormality.\",\n        ],\n        'Effusion': [\n            \"Pleural effusion noted. Otherwise clear lung fields. No pneumothorax.\",\n        ],\n        'Infiltration': [\n            \"Infiltrate present, possibly representing infection. Clinical correlation recommended.\",\n        ],\n        'Mass': [\n            \"Pulmonary mass identified. Recommend CT for further evaluation.\",\n        ],\n        'Nodule': [\n            \"Pulmonary nodule noted. Follow-up imaging recommended.\",\n        ],\n        'Pneumonia': [\n            \"Consolidation consistent with pneumonia. Clinical correlation recommended.\",\n        ],\n        'Pneumothorax': [\n            \"Pneumothorax present. Clinical correlation recommended.\",\n        ],\n        'Consolidation': [\n            \"Consolidation present. Clinical correlation recommended.\",\n        ],\n        'Edema': [\n            \"Pulmonary edema with prominent interstitial markings. Cardiomegaly present.\",\n        ],\n        'Emphysema': [\n            \"Emphysematous changes. Hyperinflation present. Heart size normal.\",\n        ],\n        'Fibrosis': [\n            \"Pulmonary fibrosis with reticular opacities. No acute process.\",\n        ],\n        'Pleural_Thickening': [\n            \"Pleural thickening. No acute abnormality. Lungs otherwise clear.\",\n        ],\n        'Hernia': [\n            \"Hiatal hernia present. Otherwise unremarkable chest radiograph.\",\n        ]\n    }\n    \n    def __init__(self, dataset_path, output_path):\n        self.dataset_path = Path(dataset_path)\n        self.output_path = Path(output_path)\n        self.output_path.mkdir(exist_ok=True)\n        \n    def generate_report(self, findings, view='PA'):\n        \"\"\"Generate report from findings - exact same logic as training\"\"\"\n        if pd.isna(findings) or findings == 'No Finding':\n            findings_list = ['No Finding']\n        else:\n            findings_list = findings.split('|')\n        \n        parts = [f\"{view} chest radiograph.\"]\n        \n        for finding in findings_list:\n            if finding in self.REPORT_TEMPLATES:\n                template = random.choice(self.REPORT_TEMPLATES[finding])\n                parts.append(template)\n        \n        report = ' '.join(parts)\n        impression = self._generate_impression(findings_list)\n        report += f\" Impression: {impression}\"\n        \n        return report\n    \n    def _generate_impression(self, findings):\n        \"\"\"Generate impression - exact same as training\"\"\"\n        if findings == ['No Finding']:\n            return \"No acute cardiopulmonary abnormality.\"\n        impressions = [f.replace('_', ' ') for f in findings if f in self.REPORT_TEMPLATES]\n        return ', '.join(impressions) + '.' if impressions else \"See findings above.\"\n    \n    def find_image(self, image_name):\n        \"\"\"Find image in dataset folders\"\"\"\n        for i in range(1, 13):\n            folder = self.dataset_path / f'images_{i:03d}' / 'images'\n            image_path = folder / image_name\n            if image_path.exists():\n                return image_path\n        return None\n    \n    def prepare_test_only(self, sample_size=None):\n        \"\"\"Prepare ONLY test dataset\"\"\"\n        print(\"\\nLoading metadata...\")\n        df = pd.read_csv(self.dataset_path / 'Data_Entry_2017.csv')\n        \n        # Load test split\n        with open(self.dataset_path / 'test_list.txt', 'r') as f:\n            test_imgs = set(line.strip() for line in f)\n        \n        # Get test data\n        test_df = df[df['Image Index'].isin(test_imgs)]\n        \n        if sample_size:\n            test_df = test_df.sample(n=min(sample_size, len(test_df)), random_state=42)\n            print(f\"Using {len(test_df)} test samples\")\n        else:\n            print(f\"Using all {len(test_df)} test samples\")\n        \n        print(f\"\\nData split:\")\n        print(f\"  Test: {len(test_df)}\")\n        \n        # Create output directory\n        (self.output_path / 'images').mkdir(exist_ok=True)\n        \n        # Process test data\n        print(f\"\\nProcessing test...\")\n        \n        data = []\n        for _, row in tqdm(test_df.iterrows(), total=len(test_df)):\n            # Generate report\n            report = self.generate_report(row['Finding Labels'], row['View Position'])\n            \n            # Find image\n            img_name = row['Image Index']\n            src = self.find_image(img_name)\n            \n            if src is None:\n                continue\n            \n            # Copy image (symlink to save space)\n            dst = self.output_path / 'images' / img_name\n            if not dst.exists():\n                # Use symlink instead of copy to save space!\n                try:\n                    os.symlink(src, dst)\n                except:\n                    shutil.copy2(src, dst)\n            \n            data.append({'image_path': img_name, 'report': report})\n        \n        # Save CSV\n        pd.DataFrame(data).to_csv(self.output_path / 'test_data.csv', index=False)\n        print(f\"  ‚úì Saved {len(data)} test samples\")\n        \n        print(f\"\\n‚úÖ Test dataset prepared in {self.output_path}\")\n        \n        #return data\n\n\n# Prepare ONLY test dataset\npreparator = NIHTestDataPreparator(config['dataset_path'], config['output_dir'])\npreparator.prepare_test_only(sample_size=config['sample_size'])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-19T05:27:10.760983Z","iopub.execute_input":"2026-01-19T05:27:10.761263Z","iopub.status.idle":"2026-01-19T05:29:11.955614Z","shell.execute_reply.started":"2026-01-19T05:27:10.761239Z","shell.execute_reply":"2026-01-19T05:29:11.954935Z"}},"outputs":[{"name":"stdout","text":"\n======================================================================\n  Preparing TEST Dataset Only\n======================================================================\n\nLoading metadata...\nUsing all 25596 test samples\n\nData split:\n  Test: 25596\n\nProcessing test...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/25596 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"865367a5ad9b44c88704160e105836ca"}},"metadata":{}},{"name":"stdout","text":"  ‚úì Saved 25596 test samples\n\n‚úÖ Test dataset prepared in /kaggle/working\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"# CELL 3: Download Model from HuggingFace\n# ================================================================\n\nprint(\"\\n\" + \"=\"*70)\nprint(\"  DOWNLOADING MODEL FROM HUGGINGFACE\")\nprint(\"=\"*70)\n\nfrom transformers import BlipProcessor, BlipForConditionalGeneration\nfrom huggingface_hub import login\n\n# Login if needed\nif HF_TOKEN:\n    login(token=HF_TOKEN)\n    print(\"‚úÖ Logged in to HuggingFace\")\n\n# Download model\nprint(f\"\\nüì• Downloading: {HUGGINGFACE_MODEL_ID}\")\n\nprocessor = BlipProcessor.from_pretrained(\n    HUGGINGFACE_MODEL_ID,\n    token=HF_TOKEN\n)\nprint(\"‚úÖ Processor loaded\")\n\nmodel = BlipForConditionalGeneration.from_pretrained(\n    HUGGINGFACE_MODEL_ID,\n    torch_dtype=torch.float16 if USE_FP16 else torch.float32,\n    token=HF_TOKEN\n)\nprint(\"‚úÖ Model loaded\")\n\n# Setup device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Use DataParallel for T4 X2\nif torch.cuda.device_count() > 1:\n    print(f\"\\nüöÄ Using {torch.cuda.device_count()} GPUs (DataParallel)\")\n    model = torch.nn.DataParallel(model)\n\nmodel = model.to(device)\nmodel.eval()\n\nprint(f\"\\n‚úÖ Model ready on {device}\")\n\n# Print model info\ntotal_params = sum(p.numel() for p in model.parameters()) / 1e6\nprint(f\"üìä Model parameters: {total_params:.1f}M\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-19T05:29:11.957071Z","iopub.execute_input":"2026-01-19T05:29:11.957301Z","iopub.status.idle":"2026-01-19T05:29:41.944940Z","shell.execute_reply.started":"2026-01-19T05:29:11.957278Z","shell.execute_reply":"2026-01-19T05:29:41.944356Z"}},"outputs":[{"name":"stdout","text":"\n======================================================================\n  DOWNLOADING MODEL FROM HUGGINGFACE\n======================================================================\n","output_type":"stream"},{"name":"stderr","text":"2026-01-19 05:29:19.325237: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1768800559.518724      55 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1768800559.574805      55 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nW0000 00:00:1768800560.034504      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1768800560.034544      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1768800560.034547      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1768800560.034549      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nUsing a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n","output_type":"stream"},{"name":"stdout","text":"\nüì• Downloading: anassaifi8912/chestxray-blip-report-generator\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"preprocessor_config.json:   0%|          | 0.00/431 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1d291e339a1947898579d120966cff59"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9743c2957d8b4e07944b6f7c9e38bc90"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d6cd80289a0d4f0fa4a87b92b9ab5974"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"104f5b3db8ab47a2bdb0884d10f6e80d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/125 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8b02ee6c4099447da7abbc04b2e6f87d"}},"metadata":{}},{"name":"stderr","text":"`torch_dtype` is deprecated! Use `dtype` instead!\n","output_type":"stream"},{"name":"stdout","text":"‚úÖ Processor loaded\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/652 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"75570dc7168a4338ad56b8e0b4607e92"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/990M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6d6cc3d2259a4ef0afee8414c3c5ebe0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/136 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4f708fc96f294642a77836c29a789e35"}},"metadata":{}},{"name":"stdout","text":"‚úÖ Model loaded\n\nüöÄ Using 2 GPUs (DataParallel)\n\n‚úÖ Model ready on cuda\nüìä Model parameters: 224.0M\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"# LOAD TEST DATA\n# ================================================================\n\nprint(\"\\nüìÇ Loading test data...\")\n\nimport pandas as pd\nfrom torch.utils.data import Dataset, DataLoader\nfrom PIL import Image\nfrom pathlib import Path\nimport torch\n\n\ntest_csv = Path(\"/kaggle/working/\") / \"test_data.csv\"\ntest_df = pd.read_csv(test_csv)\n\nprint(f\"‚úÖ Loaded {len(test_df)} test samples\")\n\n# Dataset class\nclass TestDataset(Dataset):\n    def __init__(self, df, images_dir, processor):\n        self.df = df\n        self.images_dir = Path(images_dir)\n        self.processor = processor\n    \n    def __len__(self):\n        return len(self.df)\n    \n    def __getitem__(self, idx):\n        row = self.df.iloc[idx]\n        \n        image_path = self.images_dir / row['image_path']\n        try:\n            image = Image.open(image_path).convert('RGB')\n        except:\n            image = Image.new('RGB', (384, 384), color='black')\n        \n        inputs = self.processor(images=image, return_tensors=\"pt\")\n        \n        return {\n            'pixel_values': inputs['pixel_values'].squeeze(0),\n            'reference': row['report'],\n            'image_id': row['image_path']\n        }\n\n# Create dataset and dataloader\ntest_dataset = TestDataset(\n    test_df, \n    Path(\"/kaggle/working/\") / \"images\",\n    processor\n)\n\ntest_loader = DataLoader(\n    test_dataset,\n    batch_size=BATCH_SIZE,\n    shuffle=False,\n    num_workers=4,\n    pin_memory=True\n)\n\nprint(f\"‚úÖ DataLoader ready: {len(test_loader)} batches\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-19T05:29:41.945862Z","iopub.execute_input":"2026-01-19T05:29:41.946387Z","iopub.status.idle":"2026-01-19T05:29:42.002768Z","shell.execute_reply.started":"2026-01-19T05:29:41.946361Z","shell.execute_reply":"2026-01-19T05:29:42.002216Z"}},"outputs":[{"name":"stdout","text":"\nüìÇ Loading test data...\n‚úÖ Loaded 25596 test samples\n‚úÖ DataLoader ready: 2133 batches\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"#==================Cell 8: Always ON Display ==================================\nfrom IPython.display import Javascript, display\n\n# Keep session alive by simulating activity\ndef keep_alive():\n    display(Javascript('''\n        function KeepClicking(){\n            console.log(\"Keeping session alive...\");\n            document.querySelector('body').click();\n        }\n        setInterval(KeepClicking, 60000); // Click every 60 seconds\n    '''))\n\nkeep_alive()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-19T05:29:42.004221Z","iopub.execute_input":"2026-01-19T05:29:42.004502Z","iopub.status.idle":"2026-01-19T05:29:42.009638Z","shell.execute_reply.started":"2026-01-19T05:29:42.004480Z","shell.execute_reply":"2026-01-19T05:29:42.009061Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.Javascript object>","application/javascript":"\n        function KeepClicking(){\n            console.log(\"Keeping session alive...\");\n            document.querySelector('body').click();\n        }\n        setInterval(KeepClicking, 60000); // Click every 60 seconds\n    "},"metadata":{}}],"execution_count":10},{"cell_type":"code","source":"# ================================================================\n# CELL 5: Generate Predictions (WITH CHECKPOINT SAVING)\n# ================================================================\n\n# All necessary imports\nimport time\nimport json\nfrom pathlib import Path\nfrom tqdm import tqdm\nimport torch\n\n# Fix deprecation warning - use new autocast API\ntry:\n    from torch.amp import autocast\nexcept ImportError:\n    from torch.cuda.amp import autocast\n\nprint(\"\\n\" + \"=\"*70)\nprint(\"  GENERATING PREDICTIONS\")\nprint(\"=\"*70)\n\n# Create output directory if it doesn't exist\nPath(OUTPUT_DIR).mkdir(exist_ok=True, parents=True)\n\npredictions = []\nreferences = []\nimage_ids = []\n\nstart_time = time.time()\ncheckpoint_every = 100  # Save every 100 batches\n\nprint(f\"\\nüöÄ Starting inference...\")\nprint(f\"‚ö° Checkpoints will be saved every {checkpoint_every} batches\")\n\n# FIX for DataParallel - extract the actual model\nmodel_to_use = model.module if hasattr(model, 'module') else model\n\nwith torch.no_grad():\n    for batch_idx, batch in enumerate(tqdm(test_loader, desc=\"Testing\")):\n        pixel_values = batch['pixel_values'].to(device)\n        \n        # Generate predictions\n        if USE_FP16:\n            with autocast('cuda'):  # Fixed deprecation warning\n                outputs = model_to_use.generate(\n                    pixel_values=pixel_values,\n                    max_length=128,\n                    num_beams=5,\n                    early_stopping=True\n                )\n        else:\n            outputs = model_to_use.generate(\n                pixel_values=pixel_values,\n                max_length=128,\n                num_beams=5,\n                early_stopping=True\n            )\n        \n        # Decode predictions\n        for i in range(len(outputs)):\n            pred = processor.decode(outputs[i], skip_special_tokens=True)\n            predictions.append(pred)\n            references.append(batch['reference'][i])\n            image_ids.append(batch['image_id'][i])\n        \n        # Save checkpoint every N batches\n        if (batch_idx + 1) % checkpoint_every == 0:\n            checkpoint_file = Path(OUTPUT_DIR) / f\"predictions_checkpoint_{batch_idx+1}.json\"\n            \n            # Ensure directory exists\n            checkpoint_file.parent.mkdir(exist_ok=True, parents=True)\n            \n            with open(checkpoint_file, 'w') as f:\n                json.dump({\n                    'predictions': predictions,\n                    'references': references,\n                    'image_ids': image_ids,\n                    'batches_processed': batch_idx + 1,\n                    'samples_processed': len(predictions)\n                }, f, indent=2)\n            print(f\"\\n  üíæ Checkpoint saved: {checkpoint_file.name}\")\n\ntotal_time = time.time() - start_time\n\nprint(f\"\\n‚úÖ Predictions complete!\")\nprint(f\"  Total: {len(predictions)} samples\")\nprint(f\"  Time: {total_time:.1f}s ({total_time/60:.1f} min)\")\nprint(f\"  Speed: {len(predictions)/total_time:.1f} samples/sec\")\n\n# Save final predictions immediately\npredictions_file = Path(OUTPUT_DIR) / \"predictions_final.json\"\n\n# Ensure directory exists\npredictions_file.parent.mkdir(exist_ok=True, parents=True)\n\nwith open(predictions_file, 'w') as f:\n    json.dump({\n        'predictions': predictions,\n        'references': references,\n        'image_ids': image_ids,\n        'total_samples': len(predictions),\n        'inference_time': total_time,\n        'inference_speed': len(predictions)/total_time\n    }, f, indent=2)\n\nprint(f\"\\nüíæ Final predictions saved: {predictions_file}\")\nprint(f\"üìÅ Location: {predictions_file}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-19T05:51:20.771704Z","iopub.execute_input":"2026-01-19T05:51:20.772096Z","iopub.status.idle":"2026-01-19T08:47:39.553926Z","shell.execute_reply.started":"2026-01-19T05:51:20.772060Z","shell.execute_reply":"2026-01-19T08:47:39.552932Z"}},"outputs":[{"name":"stdout","text":"\n======================================================================\n  GENERATING PREDICTIONS\n======================================================================\n\nüöÄ Starting inference...\n‚ö° Checkpoints will be saved every 100 batches\n","output_type":"stream"},{"name":"stderr","text":"Testing:   5%|‚ñç         | 100/2133 [08:15<2:48:11,  4.96s/it]","output_type":"stream"},{"name":"stdout","text":"\n  üíæ Checkpoint saved: predictions_checkpoint_100.json\n","output_type":"stream"},{"name":"stderr","text":"Testing:   9%|‚ñâ         | 200/2133 [16:31<2:40:25,  4.98s/it]","output_type":"stream"},{"name":"stdout","text":"\n  üíæ Checkpoint saved: predictions_checkpoint_200.json\n","output_type":"stream"},{"name":"stderr","text":"Testing:  14%|‚ñà‚ñç        | 300/2133 [24:48<2:31:44,  4.97s/it]","output_type":"stream"},{"name":"stdout","text":"\n  üíæ Checkpoint saved: predictions_checkpoint_300.json\n","output_type":"stream"},{"name":"stderr","text":"Testing:  19%|‚ñà‚ñâ        | 400/2133 [33:04<2:23:43,  4.98s/it]","output_type":"stream"},{"name":"stdout","text":"\n  üíæ Checkpoint saved: predictions_checkpoint_400.json\n","output_type":"stream"},{"name":"stderr","text":"Testing:  23%|‚ñà‚ñà‚ñé       | 500/2133 [41:19<2:15:34,  4.98s/it]","output_type":"stream"},{"name":"stdout","text":"\n  üíæ Checkpoint saved: predictions_checkpoint_500.json\n","output_type":"stream"},{"name":"stderr","text":"Testing:  28%|‚ñà‚ñà‚ñä       | 600/2133 [49:34<2:07:24,  4.99s/it]","output_type":"stream"},{"name":"stdout","text":"\n  üíæ Checkpoint saved: predictions_checkpoint_600.json\n","output_type":"stream"},{"name":"stderr","text":"Testing:  33%|‚ñà‚ñà‚ñà‚ñé      | 700/2133 [57:51<1:58:57,  4.98s/it]","output_type":"stream"},{"name":"stdout","text":"\n  üíæ Checkpoint saved: predictions_checkpoint_700.json\n","output_type":"stream"},{"name":"stderr","text":"Testing:  38%|‚ñà‚ñà‚ñà‚ñä      | 800/2133 [1:06:08<1:50:28,  4.97s/it]","output_type":"stream"},{"name":"stdout","text":"\n  üíæ Checkpoint saved: predictions_checkpoint_800.json\n","output_type":"stream"},{"name":"stderr","text":"Testing:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 900/2133 [1:14:24<1:42:00,  4.96s/it]","output_type":"stream"},{"name":"stdout","text":"\n  üíæ Checkpoint saved: predictions_checkpoint_900.json\n","output_type":"stream"},{"name":"stderr","text":"Testing:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 1000/2133 [1:22:41<1:33:56,  4.97s/it]","output_type":"stream"},{"name":"stdout","text":"\n  üíæ Checkpoint saved: predictions_checkpoint_1000.json\n","output_type":"stream"},{"name":"stderr","text":"Testing:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 1100/2133 [1:30:59<1:25:57,  4.99s/it]","output_type":"stream"},{"name":"stdout","text":"\n  üíæ Checkpoint saved: predictions_checkpoint_1100.json\n","output_type":"stream"},{"name":"stderr","text":"Testing:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 1200/2133 [1:39:16<1:17:42,  5.00s/it]","output_type":"stream"},{"name":"stdout","text":"\n  üíæ Checkpoint saved: predictions_checkpoint_1200.json\n","output_type":"stream"},{"name":"stderr","text":"Testing:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 1300/2133 [1:47:32<1:09:01,  4.97s/it]","output_type":"stream"},{"name":"stdout","text":"\n  üíæ Checkpoint saved: predictions_checkpoint_1300.json\n","output_type":"stream"},{"name":"stderr","text":"Testing:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 1400/2133 [1:55:48<1:01:14,  5.01s/it]","output_type":"stream"},{"name":"stdout","text":"\n  üíæ Checkpoint saved: predictions_checkpoint_1400.json\n","output_type":"stream"},{"name":"stderr","text":"Testing:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 1500/2133 [2:04:05<52:27,  4.97s/it]  ","output_type":"stream"},{"name":"stdout","text":"\n  üíæ Checkpoint saved: predictions_checkpoint_1500.json\n","output_type":"stream"},{"name":"stderr","text":"Testing:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 1600/2133 [2:12:21<44:12,  4.98s/it]","output_type":"stream"},{"name":"stdout","text":"\n  üíæ Checkpoint saved: predictions_checkpoint_1600.json\n","output_type":"stream"},{"name":"stderr","text":"Testing:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 1700/2133 [2:20:31<35:37,  4.94s/it]","output_type":"stream"},{"name":"stdout","text":"\n  üíæ Checkpoint saved: predictions_checkpoint_1700.json\n","output_type":"stream"},{"name":"stderr","text":"Testing:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 1800/2133 [2:28:48<27:43,  5.00s/it]","output_type":"stream"},{"name":"stdout","text":"\n  üíæ Checkpoint saved: predictions_checkpoint_1800.json\n","output_type":"stream"},{"name":"stderr","text":"Testing:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 1900/2133 [2:37:05<19:20,  4.98s/it]","output_type":"stream"},{"name":"stdout","text":"\n  üíæ Checkpoint saved: predictions_checkpoint_1900.json\n","output_type":"stream"},{"name":"stderr","text":"Testing:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 2000/2133 [2:45:20<11:03,  4.99s/it]","output_type":"stream"},{"name":"stdout","text":"\n  üíæ Checkpoint saved: predictions_checkpoint_2000.json\n","output_type":"stream"},{"name":"stderr","text":"Testing:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 2100/2133 [2:53:33<02:45,  5.01s/it]","output_type":"stream"},{"name":"stdout","text":"\n  üíæ Checkpoint saved: predictions_checkpoint_2100.json\n","output_type":"stream"},{"name":"stderr","text":"Testing: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2133/2133 [2:56:18<00:00,  4.96s/it]","output_type":"stream"},{"name":"stdout","text":"\n‚úÖ Predictions complete!\n  Total: 25596 samples\n  Time: 10578.7s (176.3 min)\n  Speed: 2.4 samples/sec\n\nüíæ Final predictions saved: /kaggle/working/test_results/predictions_final.json\nüìÅ Location: /kaggle/working/test_results/predictions_final.json\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":18},{"cell_type":"code","source":"# CELL 6: Calculate Metrics\n# ================================================================\nprint(\"\\n\" + \"=\"*70)\nprint(\"  CALCULATING METRICS\")\nprint(\"=\"*70)\n\nimport numpy as np\n# Install NLTK\n!pip install -q nltk rouge-score\n\nimport nltk\nnltk.download('wordnet', quiet=True)\nnltk.download('omw-1.4', quiet=True)\nnltk.download('punkt', quiet=True)\n\nfrom nltk.translate.bleu_score import corpus_bleu, SmoothingFunction\nfrom nltk.translate.meteor_score import meteor_score\nfrom rouge_score import rouge_scorer\n\n# BLEU\nprint(\"\\nüìä BLEU...\")\nsmoothing = SmoothingFunction().method1\nrefs_tok = [[ref.lower().split()] for ref in references]\npreds_tok = [pred.lower().split() for pred in predictions]\n\nbleu_1 = corpus_bleu(refs_tok, preds_tok, weights=(1,0,0,0), smoothing_function=smoothing)\nbleu_2 = corpus_bleu(refs_tok, preds_tok, weights=(0.5,0.5,0,0), smoothing_function=smoothing)\nbleu_3 = corpus_bleu(refs_tok, preds_tok, weights=(0.33,0.33,0.33,0), smoothing_function=smoothing)\nbleu_4 = corpus_bleu(refs_tok, preds_tok, weights=(0.25,0.25,0.25,0.25), smoothing_function=smoothing)\n\n# METEOR\nprint(\"üìä METEOR...\")\nmeteor_scores = []\nfor ref, pred in zip(references, predictions):\n    try:\n        score = meteor_score([ref.lower().split()], pred.lower().split())\n        meteor_scores.append(score)\n    except:\n        meteor_scores.append(0.0)\nmeteor_avg = np.mean(meteor_scores)\n\n# ROUGE-L\nprint(\"üìä ROUGE-L...\")\nscorer = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)\nrouge_scores = [scorer.score(ref, pred)['rougeL'].fmeasure for ref, pred in zip(references, predictions)]\nrouge_avg = np.mean(rouge_scores)\n\n# Clinical Accuracy\nprint(\"üìä Clinical Accuracy...\")\nclinical_terms = ['atelectasis', 'cardiomegaly', 'effusion', 'infiltration', 'mass', \n                  'nodule', 'pneumonia', 'pneumothorax', 'consolidation', 'edema',\n                  'emphysema', 'fibrosis', 'pleural', 'thickening', 'hernia', 'normal']\n\ncorrect = 0\nfor ref, pred in zip(references, predictions):\n    ref_lower, pred_lower = ref.lower(), pred.lower()\n    ref_terms = [t for t in clinical_terms if t in ref_lower]\n    pred_terms = [t for t in clinical_terms if t in pred_lower]\n    \n    if not ref_terms:\n        if not pred_terms:\n            correct += 1\n    else:\n        matching = len(set(ref_terms) & set(pred_terms))\n        if matching / len(ref_terms) >= 0.5:\n            correct += 1\n\nclinical_acc = correct / len(references)\n\nmetrics = {\n    'BLEU-1': float(bleu_1),\n    'BLEU-2': float(bleu_2),\n    'BLEU-3': float(bleu_3),\n    'BLEU-4': float(bleu_4),\n    'METEOR': float(meteor_avg),\n    'ROUGE-L': float(rouge_avg),\n    'Clinical-Accuracy': float(clinical_acc)\n}\n\nprint(\"\\n‚úÖ Metrics calculated\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-19T08:50:00.111755Z","iopub.execute_input":"2026-01-19T08:50:00.112374Z","iopub.status.idle":"2026-01-19T08:51:26.910567Z","shell.execute_reply.started":"2026-01-19T08:50:00.112338Z","shell.execute_reply":"2026-01-19T08:51:26.909732Z"}},"outputs":[{"name":"stdout","text":"\n======================================================================\n  CALCULATING METRICS\n======================================================================\n\nüìä BLEU...\nüìä METEOR...\nüìä ROUGE-L...\nüìä Clinical Accuracy...\n\n‚úÖ Metrics calculated\n","output_type":"stream"}],"execution_count":20},{"cell_type":"code","source":"# CELL 7: Display & Save Results\n# ================================================================\nprint(\"\\n\" + \"=\"*70)\nprint(\"  FINAL TEST RESULTS\")\nprint(\"=\"*70)\n\nprint(\"\\nüìà Metrics:\")\nfor metric, value in metrics.items():\n    print(f\"  {metric:20s}: {value:.4f}\")\n\n# Interpretation\nprint(f\"\\nüí° Performance:\")\nif metrics['BLEU-4'] > 0.30:\n    print(\"  ‚úÖ Excellent BLEU-4\")\nelif metrics['BLEU-4'] > 0.20:\n    print(\"  ‚úÖ Good BLEU-4\")\nelse:\n    print(\"  ‚ö†Ô∏è BLEU-4 needs improvement\")\n\nif metrics['Clinical-Accuracy'] > 0.70:\n    print(\"  ‚úÖ Excellent clinical accuracy\")\nelif metrics['Clinical-Accuracy'] > 0.60:\n    print(\"  ‚úÖ Good clinical accuracy\")\nelse:\n    print(\"  ‚ö†Ô∏è Clinical accuracy needs improvement\")\n\n# Save metrics\nmetrics_file = Path(OUTPUT_DIR) / \"test_metrics.json\"\nwith open(metrics_file, 'w') as f:\n    json.dump(metrics, f, indent=2)\nprint(f\"\\nüíæ Metrics saved: {metrics_file}\")\n\n# Save detailed predictions\ndetailed_file = Path(OUTPUT_DIR) / \"test_predictions_detailed.json\"\nresults_detail = []\nfor i in range(len(predictions)):\n    results_detail.append({\n        'image_id': image_ids[i],\n        'reference': references[i],\n        'prediction': predictions[i]\n    })\n\nwith open(detailed_file, 'w') as f:\n    json.dump(results_detail, f, indent=2)\nprint(f\"üíæ Detailed predictions: {detailed_file}\")\n\n# Save sample predictions\nsample_file = Path(OUTPUT_DIR) / \"sample_predictions.txt\"\nwith open(sample_file, 'w') as f:\n    f.write(\"=\"*70 + \"\\n\")\n    f.write(\"TEST SET - SAMPLE PREDICTIONS\\n\")\n    f.write(\"=\"*70 + \"\\n\\n\")\n    \n    for i in range(min(20, len(results_detail))):\n        result = results_detail[i]\n        f.write(f\"Sample {i+1}:\\n\")\n        f.write(f\"Image: {result['image_id']}\\n\\n\")\n        f.write(f\"Reference:\\n{result['reference']}\\n\\n\")\n        f.write(f\"Prediction:\\n{result['prediction']}\\n\")\n        f.write(\"\\n\" + \"-\"*70 + \"\\n\\n\")\n\nprint(f\"üíæ Sample predictions: {sample_file}\")\n\n# Create summary report\nreport_file = Path(OUTPUT_DIR) / \"test_report.txt\"\nwith open(report_file, 'w') as f:\n    f.write(\"=\"*70 + \"\\n\")\n    f.write(\"BLIP CHEST X-RAY MODEL - TEST EVALUATION REPORT\\n\")\n    f.write(\"=\"*70 + \"\\n\\n\")\n    \n    f.write(f\"Test samples: {len(predictions)}\\n\")\n    f.write(f\"Inference time: {total_time:.1f}s ({total_time/60:.1f} min)\\n\")\n    f.write(f\"Inference speed: {len(predictions)/total_time:.1f} samples/sec\\n\")\n    f.write(f\"Device: {torch.cuda.device_count()} x GPU\\n\\n\")\n    \n    f.write(\"METRICS:\\n\")\n    f.write(\"-\"*70 + \"\\n\")\n    for metric, value in metrics.items():\n        f.write(f\"{metric:20s}: {value:.4f}\\n\")\n    \n    f.write(\"\\n\" + \"=\"*70 + \"\\n\")\n    f.write(\"FINAL EVALUATION - These are your official test results\\n\")\n    f.write(\"=\"*70 + \"\\n\")\n\nprint(f\"üíæ Test report: {report_file}\")\n\nprint(\"\\n\" + \"=\"*70)\nprint(\"  ‚úÖ EVALUATION COMPLETE!\")\nprint(\"=\"*70)\n\nprint(f\"\\nüìÅ All files saved to: {OUTPUT_DIR}\")\nprint(f\"\\nüìÑ Files created:\")\nprint(f\"  1. test_metrics.json - All metric scores\")\nprint(f\"  2. test_predictions_detailed.json - Every prediction\")\nprint(f\"  3. sample_predictions.txt - 20 examples for review\")\nprint(f\"  4. test_report.txt - Comprehensive summary\")\nprint(f\"  5. predictions_final.json - Raw predictions data\")\nprint(f\"  6. predictions_checkpoint_*.json - Checkpoints (if any)\")\n\nprint(f\"\\n‚ú® Test evaluation successful!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-19T08:51:33.450932Z","iopub.execute_input":"2026-01-19T08:51:33.451758Z","iopub.status.idle":"2026-01-19T08:51:33.644645Z","shell.execute_reply.started":"2026-01-19T08:51:33.451715Z","shell.execute_reply":"2026-01-19T08:51:33.644031Z"}},"outputs":[{"name":"stdout","text":"\n======================================================================\n  FINAL TEST RESULTS\n======================================================================\n\nüìà Metrics:\n  BLEU-1              : 0.1019\n  BLEU-2              : 0.0692\n  BLEU-3              : 0.0341\n  BLEU-4              : 0.0189\n  METEOR              : 0.1692\n  ROUGE-L             : 0.1803\n  Clinical-Accuracy   : 0.3159\n\nüí° Performance:\n  ‚ö†Ô∏è BLEU-4 needs improvement\n  ‚ö†Ô∏è Clinical accuracy needs improvement\n\nüíæ Metrics saved: /kaggle/working/test_results/test_metrics.json\nüíæ Detailed predictions: /kaggle/working/test_results/test_predictions_detailed.json\nüíæ Sample predictions: /kaggle/working/test_results/sample_predictions.txt\nüíæ Test report: /kaggle/working/test_results/test_report.txt\n\n======================================================================\n  ‚úÖ EVALUATION COMPLETE!\n======================================================================\n\nüìÅ All files saved to: /kaggle/working/test_results\n\nüìÑ Files created:\n  1. test_metrics.json - All metric scores\n  2. test_predictions_detailed.json - Every prediction\n  3. sample_predictions.txt - 20 examples for review\n  4. test_report.txt - Comprehensive summary\n  5. predictions_final.json - Raw predictions data\n  6. predictions_checkpoint_*.json - Checkpoints (if any)\n\n‚ú® Test evaluation successful!\n","output_type":"stream"}],"execution_count":21},{"cell_type":"code","source":"pip install kaggle\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-19T09:05:23.382651Z","iopub.execute_input":"2026-01-19T09:05:23.383643Z","iopub.status.idle":"2026-01-19T09:05:26.440521Z","shell.execute_reply.started":"2026-01-19T09:05:23.383607Z","shell.execute_reply":"2026-01-19T09:05:26.439768Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: kaggle in /usr/local/lib/python3.12/dist-packages (1.7.4.5)\nRequirement already satisfied: bleach in /usr/local/lib/python3.12/dist-packages (from kaggle) (6.2.0)\nRequirement already satisfied: certifi>=14.05.14 in /usr/local/lib/python3.12/dist-packages (from kaggle) (2026.1.4)\nRequirement already satisfied: charset-normalizer in /usr/local/lib/python3.12/dist-packages (from kaggle) (3.4.4)\nRequirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from kaggle) (3.11)\nRequirement already satisfied: protobuf in /usr/local/lib/python3.12/dist-packages (from kaggle) (5.29.5)\nRequirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.12/dist-packages (from kaggle) (2.9.0.post0)\nRequirement already satisfied: python-slugify in /usr/local/lib/python3.12/dist-packages (from kaggle) (8.0.4)\nRequirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from kaggle) (2.32.5)\nRequirement already satisfied: setuptools>=21.0.0 in /usr/local/lib/python3.12/dist-packages (from kaggle) (75.2.0)\nRequirement already satisfied: six>=1.10 in /usr/local/lib/python3.12/dist-packages (from kaggle) (1.17.0)\nRequirement already satisfied: text-unidecode in /usr/local/lib/python3.12/dist-packages (from kaggle) (1.3)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from kaggle) (4.67.1)\nRequirement already satisfied: urllib3>=1.15.1 in /usr/local/lib/python3.12/dist-packages (from kaggle) (2.6.3)\nRequirement already satisfied: webencodings in /usr/local/lib/python3.12/dist-packages (from kaggle) (0.5.1)\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":22},{"cell_type":"code","source":"!zip -r /kaggle/working/test_results.zip /kaggle/working/test_results\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-19T09:09:48.261587Z","iopub.execute_input":"2026-01-19T09:09:48.261888Z","iopub.status.idle":"2026-01-19T09:09:51.007960Z","shell.execute_reply.started":"2026-01-19T09:09:48.261862Z","shell.execute_reply":"2026-01-19T09:09:51.007278Z"}},"outputs":[{"name":"stdout","text":"  adding: kaggle/working/test_results/ (stored 0%)\n  adding: kaggle/working/test_results/predictions_checkpoint_400.json (deflated 97%)\n  adding: kaggle/working/test_results/predictions_checkpoint_900.json (deflated 97%)\n  adding: kaggle/working/test_results/predictions_checkpoint_1500.json (deflated 97%)\n  adding: kaggle/working/test_results/sample_predictions.txt (deflated 92%)\n  adding: kaggle/working/test_results/predictions_checkpoint_1800.json (deflated 97%)\n  adding: kaggle/working/test_results/predictions_checkpoint_300.json (deflated 97%)\n  adding: kaggle/working/test_results/predictions_checkpoint_1100.json (deflated 97%)\n  adding: kaggle/working/test_results/predictions_checkpoint_200.json (deflated 97%)\n  adding: kaggle/working/test_results/predictions_checkpoint_1300.json (deflated 97%)\n  adding: kaggle/working/test_results/predictions_checkpoint_2000.json (deflated 97%)\n  adding: kaggle/working/test_results/predictions_checkpoint_500.json (deflated 97%)\n  adding: kaggle/working/test_results/predictions_final.json (deflated 97%)\n  adding: kaggle/working/test_results/predictions_checkpoint_700.json (deflated 97%)\n  adding: kaggle/working/test_results/predictions_checkpoint_800.json (deflated 97%)\n  adding: kaggle/working/test_results/predictions_checkpoint_600.json (deflated 97%)\n  adding: kaggle/working/test_results/predictions_checkpoint_1000.json (deflated 97%)\n  adding: kaggle/working/test_results/predictions_checkpoint_1400.json (deflated 97%)\n  adding: kaggle/working/test_results/predictions_checkpoint_1700.json (deflated 97%)\n  adding: kaggle/working/test_results/predictions_checkpoint_2100.json (deflated 97%)\n  adding: kaggle/working/test_results/predictions_checkpoint_1900.json (deflated 97%)\n  adding: kaggle/working/test_results/predictions_checkpoint_1200.json (deflated 97%)\n  adding: kaggle/working/test_results/test_metrics.json (deflated 35%)\n  adding: kaggle/working/test_results/test_report.txt (deflated 65%)\n  adding: kaggle/working/test_results/test_predictions_detailed.json (deflated 97%)\n  adding: kaggle/working/test_results/predictions_checkpoint_1600.json (deflated 97%)\n  adding: kaggle/working/test_results/predictions_checkpoint_100.json (deflated 97%)\n","output_type":"stream"}],"execution_count":26},{"cell_type":"code","source":"!ls -lh /kaggle/working/test_results.zip\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-19T09:10:10.778370Z","iopub.execute_input":"2026-01-19T09:10:10.779107Z","iopub.status.idle":"2026-01-19T09:10:10.946624Z","shell.execute_reply.started":"2026-01-19T09:10:10.779065Z","shell.execute_reply":"2026-01-19T09:10:10.945940Z"}},"outputs":[{"name":"stdout","text":"-rw-r--r-- 1 root root 6.5M Jan 19 09:09 /kaggle/working/test_results.zip\n","output_type":"stream"}],"execution_count":27}]}